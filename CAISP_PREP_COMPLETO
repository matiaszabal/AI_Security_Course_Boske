**PROGRAMA TÉCNICO: INGENIERÍA EN AI SECURITY (CAISP ALIGNED)**

Este programa está diseñado para cubrir la totalidad del temario de certificación CAISP, integrando una profundidad académica de nivel doctorado con la aplicación práctica necesaria en entornos defensivos y ofensivos de 2026.

---

### CAPÍTULO 1: INTRODUCCIÓN A LA SEGURIDAD EN IA

**Sesión 1: Fundamentos y Evolución**

* **Contenido:** Definición técnica de IA vs. ML. Historia desde sistemas expertos hasta redes neuronales profundas. Taxonomía de IA: Estrecha (ANI) vs. General (AGI).
* **Recurso:** [Elements of AI - University of Helsinki](https://www.elementsofai.com/).
* **Laboratorio:** Configuración de entorno Linux y contenedores Docker para herramientas de IA.

**Sesión 2: Paradigmas de Aprendizaje y Visión**

* **Contenido:** Análisis de aprendizaje Supervisado, No Supervisado y por Refuerzo (RL). Fundamentos de Computer Vision y procesamiento de tensores.
* **Recurso:** [Machine Learning Crash Course - Google](https://developers.google.com/machine-learning/crash-course).
* **Laboratorio:** Clasificación de texto e imágenes utilizando **TensorFlow** y **Duckling** para normalización de entidades.

**Sesión 3: Arquitecturas de Deep Learning**

* **Contenido:** Redes Neuronales Convolucionales (CNN) y arquitecturas de transformación. Componentes centrales: Datos, Algoritmos y Cómputo (GPU/TPU).
* **Recurso:** [Deep Learning Specialization - Andrew Ng (Modo Auditoría)](https://www.coursera.org/specializations/deep-learning).
* **Laboratorio:** Despliegue de **Invoke AI** para análisis de generación de imágenes y seguridad en modelos generativos locales.

---

### CAPÍTULO 2: UNDERSTANDING AND ATTACKING LLMS

**Sesión 4: Mecánica de los Modelos de Lenguaje**

* **Contenido:** Arquitecturas GPT (Decoder) y BERT (Encoder). Funcionamiento de la auto-atención (Self-attention). Diferencias entre Foundational y Fine-tuned models.
* **Recurso:** [The Illustrated Transformer - Jay Alammar](https://jalammar.github.io/illustrated-transformer/).
* **Laboratorio:** Implementación de un sistema RAG (Retrieval Augmented Generation) básico con LangChain.

**Sesión 5: Framework MITRE ATLAS**

* **Contenido:** Mapeo de tácticas: Reconocimiento, Acceso Inicial, Evasión de Defensa, Exfiltración e Impacto. Uso de la matriz ATLAS para incidentes reales.
* **Recurso:** [MITRE ATLAS Matrix](https://atlas.mitre.org/matrices/ATLAS).
* **Laboratorio:** Escaneo de vulnerabilidades en agentes de IA mediante el framework **Garak**.

**Sesión 6: Herramientas Adversarias y Robustez**

* **Contenido:** Análisis de "Dark AI" (WormGPT, FraudGPT). Técnicas de esteganografía en modelos (StegnoGAN).
* **Recurso:** [Adversarial Robustness Toolbox (ART)](https://github.com/Trusted-AI/adversarial-robustness-toolbox).
* **Laboratorio:** Ejecución de ataques de caja negra sobre modelos de texto utilizando **TextAttack**.

---

### CAPÍTULO 3: LLM TOP 10 VULNERABILITIES (OWASP)

**Sesión 7: Inyección de Prompts y Salidas Inseguras**

* **Contenido:** Prompt Injection directa e indirecta. Riesgos de ejecución de código mediante Insecure Output Handling.
* **Recurso:** [OWASP Top 10 for LLM Applications](https://owasp.org/www-project-top-10-for-large-language-model-applications/).
* **Laboratorio:** Bypass de System Prompts en aplicaciones web protegidas por guardrails débiles.

**Sesión 8: Envenenamiento y Denegación de Servicio**

* **Contenido:** Training Data Poisoning en fases de pre-entrenamiento y fine-tuning. Model DoS mediante agotamiento de ventana de contexto.
* **Recurso:** [NIST AI 100-2: Adversarial Machine Learning](https://csrc.nist.gov/pubs/ai/100/2/e2023/final).
* **Laboratorio:** Simulación de agotamiento de recursos (DoS) mediante prompts recursivos y de alta densidad.

**Sesión 9: Agencia, Privacidad y Robo**

* **Contenido:** Excessive Agency en agentes autónomos. Sensitive Information Disclosure. Model Theft (Extracción de pesos y destilación).
* **Recurso:** [Membership Inference Attacks on ML Models](https://arxiv.org/abs/1610.05820).
* **Laboratorio:** Exfiltración de secretos corporativos a través de inyecciones en plugins conectados.

---

### CAPÍTULO 4: AI ATTACKS AND DEFENSES USING DEVOPS

**Sesión 10: MLSecOps y Pipelines de IA**

* **Contenido:** Integración de seguridad en el ciclo de vida de desarrollo de IA. Vulnerabilidades en Hugging Face y plataformas de modelos.
* **Recurso:** [MLSecOps Principles - MLSecOps.com](https://mlsecops.com/).
* **Laboratorio:** Ataque de **Poisoned Pipeline** mediante el compromiso de un script de pre-procesamiento en GitHub Actions.

**Sesión 11: Herramientas de Análisis y Firewalls**

* **Contenido:** Software Composition Analysis (SCA) aplicado a modelos. Análisis estático (SAST) de archivos de pesos (SafeTensors vs. Pickle).
* **Recurso:** [Checkov for AI Infrastructure](https://www.checkov.io/).
* **Laboratorio:** Implementación de **NVIDIA NeMo Guardrails** para filtrar entradas y salidas en tiempo real.

---

### CAPÍTULO 5: THREAT MODELING AI SYSTEMS

**Sesión 12: Metodología STRIDE y DFD**

* **Contenido:** Adaptación de STRIDE a activos de IA. Creación de Data Flow Diagrams (DFD) para arquitecturas LLM.
* **Recurso:** [Microsoft Threat Modeling Tool for AI](https://learn.microsoft.com/en-us/azure/security/develop/threat-modeling-tool).
* **Laboratorio:** Modelado de amenazas de un sistema financiero basado en IA utilizando **StrideGPT**.

**Sesión 13: Bibliotecas de Riesgos y Rating**

* **Contenido:** Uso de BIML Risk Framework y AI Incident Database. Metodologías de rating de riesgo (DREAD aplicado a IA).
* **Recurso:** [AI Incident Database](https://incidentdatabase.ai/).
* **Laboratorio:** Evaluación y priorización de riesgos para una infraestructura de salud asistida por agentes.

---

### CAPÍTULO 6: SUPPLY CHAIN ATTACKS IN AI

**Sesión 14: Integridad de la Cadena de Suministro**

* **Contenido:** Ataques basados en infraestructura y modelos de terceros. Package masquerading en repositorios de ML.
* **Recurso:** [SLSA Framework for AI Supply Chain](https://slsa.dev/).
* **Laboratorio:** Detección de backdoors en archivos de modelos mediante el uso de **BackdoorBox**.

**Sesión 15: SBOM y Firmas Digitales**

* **Contenido:** Generación de Software Bill of Materials (SBOM) para modelos. Uso de MLBOMs y firmas con **CoSign**.
* **Recurso:** [CISA SBOM Guide](https://www.cisa.gov/sbom).
* **Laboratorio:** Firmado digital de un modelo y verificación de integridad antes del despliegue en un clúster de Kubernetes.

---

### CAPÍTULO 7: GOVERNANCE, COMPLIANCE AND EMERGING THREATS

**Sesión 16: Gusanos de IA y Amenazas Futuras**

* **Contenido:** Gusanos autopropagados en ecosistemas de agentes (Morris II). Backdoors en fine-tuning distribuido.
* **Recurso:** [ComPromptMized: LLM-based Worms](https://arxiv.org/abs/2403.02817).
* **Laboratorio:** Análisis forense de una infección por gusano en un sistema de agentes interconectados.

**Sesión 17: Marcos Regulatorios y Estándares**

* **Contenido:** Implementación técnica del **NIST AI RMF 1.0** e **ISO/IEC 42001**. Cumplimiento del **EU AI Act**.
* **Recurso:** [NIST AI Risk Management Framework](https://www.nist.gov/itl/ai-risk-management-framework).
* **Laboratorio:** Auditoría de cumplimiento técnico de un sistema de IA "de alto riesgo" según los criterios de la Unión Europea.

---

### PREPARACIÓN FINAL: PROCESO DE CERTIFICACIÓN CAISP

**Simulación de Examen**

* **Formato:** Laboratorio práctico de 6 horas.
* **Objetivo:** Identificar vulnerabilidades OWASP en un entorno productivo, realizar el modelado de amenazas bajo STRIDE y securizar el pipeline de despliegue.
* **Siguiente paso:** Registro en la plataforma de examen de Practical DevSecOps para la validación de credenciales.

¿Desea que profundice en la guía técnica del primer laboratorio de la **Sesión 10 (Poisoned Pipeline Attack)** para que pueda comenzar a documentar esta vulnerabilidad en su portafolio?
