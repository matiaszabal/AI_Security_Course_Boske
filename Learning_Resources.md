RECURSOS DE APRENDIZAJE GRATUITOS POR MODULO TECNICO

MODULO 1: TAXONOMIA Y ATAQUES ADVERSARIALES (VISION)

Marcos de Trabajo y Teoria:

- MITRE ATLAS Matrix: Repositorio oficial de tacticas y tecnicas de ataque a sistemas de IA. Es la base para cualquier modelado de amenazas.
- Stanford CS231n - Lecture 16: Sesion especifica sobre Adversarial Examples dictada por investigadores de Stanford (disponible en YouTube).
- ArXiv Paper - Explaining and Harnessing Adversarial Examples: El paper fundacional de Ian Goodfellow para entender el metodo FGSM.

Laboratorios y Codigo:

- IBM Adversarial Robustness Toolbox (ART) Notebooks: Repositorio en GitHub con ejemplos listos para ejecutar de ataques FGSM, PGD y DeepFool en PyTorch y TensorFlow.
- MadryLab - MNIST/CIFAR Challenge: Tutoriales y desafios sobre entrenamiento robusto y defensa ante perturbaciones.

MODULO 2: SEGURIDAD EN MODELOS DE LENGUAJE (LLM SECURITY)

Guias y Estandares:

- OWASP Top 10 for LLM Applications: Documento detallado con ejemplos de vulnerabilidades, impacto y mitigacion. Es el estandar de la industria.
- Learn Prompting - Reliability & Safety Section: Curso gratuito que cubre desde inyeccion basica hasta tecnicas avanzadas de Jailbreaking.

Plataformas de Practica (Wargames):

- Gandalf by Lakera: Juego interactivo de niveles progresivos para practicar el bypass de filtros en LLMs.
- TensorTrust: Simulador de inyeccion de prompts donde se debe enga√±ar a una IA para extraer una clave secreta.
- PromptHack (GitHub): Repositorio con diversos payloads de ataque para evaluar la robustez de modelos locales.

Investigacion de Vanguardia:

- Lilian Weng Blog - Adversarial Attacks on LLMs: Post de alta calidad tecnica de la directora de sistemas de OpenAI sobre la seguridad de los modelos de lenguaje.

MODULO 3: SECMLOPS Y RED TEAMING

Herramientas de Auditoria y Defensa:

- Microsoft PyRIT (Python Risk Identification Tool): Framework de codigo abierto en GitHub para automatizar el Red Teaming en modelos generativos.
- Garak (LLM Vulnerability Scanner): Herramienta para escanear modelos de lenguaje en busca de alucinaciones, jailbreaks y fallos de seguridad.
- NVIDIA NeMo Guardrails: Codigo y documentacion para implementar capas de defensa programaticas en aplicaciones de IA.

Seguridad en la Cadena de Suministro:

- Trail of Bits - ML Security Guide: Serie de articulos tecnicos sobre vulnerabilidades en formatos de archivos (Pickle) y seguridad en infraestructura de ML.
- Hugging Face Security Lab Blog: Publicaciones constantes sobre deteccion de malware en modelos y mejores practicas en SecMLOps.

Repositorios de Casos de Estudio:

- AI Incident Database: Base de datos global con reportes reales de fallos, ataques y brechas de seguridad en sistemas de inteligencia artificial desplegados en produccion.

ESTRATEGIA DE ESTUDIO SUGERIDA

Para maximizar el rendimiento de la cohorte, se recomienda que los alumnos utilicen los Notebooks de IBM ART para el Modulo 1, superen los niveles de Gandalf para el Modulo 2 y configuren el framework PyRIT de Microsoft para el Modulo 3. Estos recursos cubren el espectro completo desde la investigacion academica hasta la implementacion industrial.
